{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB DATA ANALYTICS PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import csv\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing lists -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "filter_links = []\n",
    "final_links = []\n",
    "final_links2 = []\n",
    "name = []\n",
    "quarter = []\n",
    "year = []\n",
    "sentiment_final_score = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding earning calls links - (using selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "##### Using selenium to scrap links of Earning Calls #####\n",
    "## We tried using beautiful soup, however, we were blocked. Hence, using selenium to scrape the web.\n",
    "\n",
    "for page_num in range(1,3500):\n",
    "    \n",
    "    url = 'https://seekingalpha.com/earnings/earnings-call-transcripts/' + str(page_num)\n",
    "    browser = webdriver.Firefox(executable_path = 'C:/Users/snigb/Desktop/Web Data Analytics/Class 7/geckodriver.exe')\n",
    "    browser.get(url)\n",
    "    \n",
    "    i = 0\n",
    "    for single_List_Item in browser.find_elements_by_xpath(\"//ul[@class='list-group sa-base-article-list']/li\"):\n",
    "        i = i+1\n",
    "        \n",
    "    for x in range(1,i+1):\n",
    "        xpath = '//*[@id=\"analysis-list-container\"]/ul/li[' + str(x) + ']/h3/a'\n",
    "        dropdown = browser.find_element_by_xpath(xpath)\n",
    "        href_data = dropdown.get_attribute('href')\n",
    "        links.append(href_data)\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering links for required quarters - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Using specific quarters to conduct the analysis and filtering for those quarters\n",
    "\n",
    "quarters = [\"q3-2017\",\"q2-2017\",\"q1-2017\",\"q4-2016\",\"q3-2016\",\"q2-2016\",\"q1-2016\",\"q4-2015\",\"q3-2015\",\n",
    "            \"q2-2015\",\"q1-2015\",\"q4-2014\",\"q3-2014\",\"q2-2014\",\"q1-2014\",\"q4-2013\",\"q3-2013\",\"q2-2013\",\n",
    "            \"q1-2013\",\"q4-2012\",\"q3-2012\",\"q2-2012\",\"q1-2012\"]\n",
    "\n",
    "filter_links += [link for link in links for quarter in quarters if quarter in link]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding 13-F mutual funds and the respective companies and filtering links for those companies -\n",
    "\n",
    "For this purpose a separate code has been written called Reading_13F. This code reads all the 13Fs (.txt and .xml) from the sec.gov website and puts them into a dataframe All_Mutual_Funds_Final.csv.\n",
    "\n",
    "However, for our analysis we are filtering for the companies below as these companies have uniform data present across the years which makes it relevant and apt for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Using specific companies to conduct the analysis and filtering for those companies\n",
    "\n",
    "final_companies = [\"alaska-air\", \"southwest-airlines\", \"jetblue\", \"delta-air-lines\", \"american-airlines\",\"coca-cola-ko\",\n",
    "                   \"coca-cola-enterprises\", \"pepsico\", \"pfizer\",\"wal-mart\", \"united-parcel-service\", \"fedex\", \"verizon\", \n",
    "                   \"at-and-t\", \"procter-and-gamble\", \"johnson-and-johnsons\",\"aapl\", \"apples-ceo\", \"comcast\", \"mmm\"]\n",
    "\n",
    "for link in filter_links:\n",
    "    for company in final_companies:\n",
    "        if company in link:\n",
    "            final_links.append(link)\n",
    "\n",
    "final_links = list(set(final_links))\n",
    "\n",
    "for i in range(0,len(final_links)):\n",
    "    if \"results\" in final_links[i]:\n",
    "        final_links2.append(final_links[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping earning calls from seeking alpha - (using beautiful soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import bs4 as bs\n",
    "from bs4 import SoupStrainer\n",
    "import time\n",
    "from urllib import FancyURLopener\n",
    "from random import choice\n",
    "\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "    'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### One methodology of scraping for Earning Calls using Beautiful Soup #####\n",
    "## We tried using beautiful soup to scrap the earning calls, however, we were blocked. Hence, using selenium.\n",
    "\n",
    "''''class MyOpener(FancyURLopener, object):\n",
    "    version = choice(user_agents)\n",
    "\n",
    "count =0\n",
    "for link in final_links2:\n",
    "    myopener = MyOpener()\n",
    "    page=myopener.open(link)\n",
    "    html = page.read()\n",
    "\n",
    "    index = link.find(\"-\")\n",
    "    link = link[index+1:]\n",
    "    index = link.find(\"-\")\n",
    "    name.append(link[:index])\n",
    "    index = link.find(\"results\")\n",
    "    quarter.append(link[index-8:index-6])\n",
    "    year.append(link[index-5:index-1])\n",
    "\n",
    "    soup = bs.BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    text = []\n",
    "\n",
    "\n",
    "    infotable = soup.find_all(\"div\", class_=\"sa-art article-width\")\n",
    "\n",
    "    for row in infotable:\n",
    "        a = row.find_all(\"p\", class_=\"p\")\n",
    "        for i in range(0,len(a)):\n",
    "            text.append(a[i].getText())''''\n",
    "\n",
    "##### Using selenium to scrap for Earning Calls #####\n",
    "\n",
    "browser = webdriver.Firefox(executable_path = 'C:/Users/snigb/Desktop/Web Data Analytics/Class 7/geckodriver.exe')\n",
    "url = 'https://seekingalpha.com/account/login'\n",
    "browser.get(url)\n",
    "USERNAME = browser.find_element_by_xpath(\"\"\"//*[@id=\"login_user_email\"]\"\"\")\n",
    "USERNAME.send_keys(\"singh488@purdue.edu\")\n",
    "\n",
    "PASSWORD = browser.find_element_by_xpath(\"\"\"//*[@id=\"login_user_password\"]\"\"\")\n",
    "PASSWORD.send_keys(\"chaitchelsea5\")\n",
    "\n",
    "Login_Button=browser.find_element_by_xpath(\"\"\"//*[@id=\"orthodox_login\"]/div[5]/input\"\"\")\n",
    "Login_Button.click()\n",
    "\n",
    "for i in range(0, len(final_links2)):\n",
    "    text = []\n",
    "    browser.get(final_links2[i])\n",
    "    dropdown = browser.find_element_by_xpath('//*[@id=\"main_content\"]')\n",
    "    text.append(dropdown.text)\n",
    "    \n",
    "    earning_Call= ''.join(text)\n",
    "    index = earning_Call.find(\"All other use is prohibited\")\n",
    "\n",
    "    earning_Call = earning_Call[:index]\n",
    "\n",
    "##### Using Text Blob for conducting sentiment analysis #####\n",
    "## This library is used as it is not only simple to understand, but also gives better results as compared to other methods\n",
    "    \n",
    "    blob=TextBlob(earning_Call)\n",
    "\n",
    "    sentiment_score=[]\n",
    "    for sentence in blob.sentences:\n",
    "        sentiment_score.append(sentence.sentiment.polarity)\n",
    "\n",
    "    sentiment_score_df=pd.DataFrame(sentiment_score,columns=['Score'])\n",
    "    sentiment_final_score.append(sentiment_score_df.Score.mean())\n",
    "\n",
    "name_sentiment = pd.DataFrame()\n",
    "name_sentiment = pd.DataFrame({'Name': name, 'Quarter': quarter, 'Year' : year, 'Sentiment_Score' : sentiment_final_score})\n",
    "name_sentiment.to_csv(\"Senti_Scores_Final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another methodology of conducting sentiment analysis - (using google cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Demonstrates how to make a simple call to the Natural Language API #####\n",
    "\n",
    "\"\"\"\"import argparse\n",
    "\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "\n",
    "def print_result(annotations):\n",
    "    score = annotations.document_sentiment.score\n",
    "    magnitude = annotations.document_sentiment.magnitude\n",
    "\n",
    "    for index, sentence in enumerate(annotations.sentences):\n",
    "        sentence_sentiment = sentence.sentiment.score\n",
    "        print('Sentence {} has a sentiment score of {}'.format(\n",
    "            index, sentence_sentiment))\n",
    "\n",
    "    print('Overall Sentiment: score of {} with magnitude of {}'.format(\n",
    "        score, magnitude))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def analyze(movie_review_filename):\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    with open(movie_review_filename, 'r') as review_file:\n",
    "        # Instantiates a plain text document.\n",
    "        content = review_file.read()\n",
    "\n",
    "    document = types.Document(\n",
    "        content=content,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "    annotations = client.analyze_sentiment(document=document)\n",
    "\n",
    "    # Print the results\n",
    "    print_result(annotations)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=__doc__,\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'movie_review_filename',\n",
    "        help='The filename of the movie review you\\'d like to analyze.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    analyze(args.movie_review_filename)\n",
    "\n",
    "analyze('earningCall_j_q3.txt')\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Market Value and Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"All_Mutual_Funds.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"Quarter\"] = pd.DatetimeIndex(df['Date']).quarter\n",
    "df[\"Year\"] = pd.DatetimeIndex(df[\"Date\"]).year\n",
    "df[\"Market Value\"] = df[\"Market Value\"].str.replace(\",\", \"\").astype(int)\n",
    "df[\"Quantity\"] = df[\"Quantity\"].str.replace(\",\", \"\").astype(int)\n",
    "df[\"Company\"] = df[\"Company\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_list=[]\n",
    "main_list = df.groupby([\"Mutual_fund\",\"Company\",\"Year\", \"Quarter\",'Date'], as_index=False)\n",
    "\n",
    "main_list2 = main_list[\"Market Value\"].mean()\n",
    "main_list3 = main_list[\"Quantity\"].mean()\n",
    "main_df = pd.merge(main_list2, main_list3, on=['Mutual_fund',\"Company\", 'Year', 'Quarter', 'Date'])\n",
    "\n",
    "main_df['Company'], main_df['Company2'] = main_df['Company'].str.split(' ', 1).str\n",
    "main_df['Company'], main_df['Company3'] = main_df['Company'].str.split('-', 1).str\n",
    "main_df['Company'], main_df['Company4'] = main_df['Company'].str.split(',', 1).str\n",
    "main_df = main_df.drop(['Company2','Company3','Company4'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df.to_csv(\"All_Mutual_Funds_Final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating S&P 500 stock prices of companies -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp = pd.read_csv(\"S&P 500.csv\")\n",
    "sp[\"Company Name\"] = sp[\"Company Name\"].str.lower()\n",
    "sp['Company'], df['Company2'] = sp['Company Name'].str.split(' ', 1).str\n",
    "sp = sp.drop(['Company Name','Month'], 1)\n",
    "#sp.to_csv(\"S&P500.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE - \n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
